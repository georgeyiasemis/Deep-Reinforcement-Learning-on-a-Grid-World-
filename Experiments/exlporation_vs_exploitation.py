import sys
sys.path.insert(1, '../')

import numpy as np
import torch
import random
import matplotlib.pyplot as plt

# Import the environment module1
from environment import Environment
# Import agent and DQN
from agent import Agent
from deep_Qlearning import DQN


# =============================================================================
# Exploration vs Exploitation
# =============================================================================

# seed = 0
# np.random.seed(seed); torch.manual_seed(seed); random.seed(seed)


# Decay factor range
deltas = np.linspace(0, 1, 20)

# Store average rewards for each decay factor
rewards = []


gamma = 0.7
batch_size = 50

update_target_every_steps = 20

# Number of episodes
num_episodes = 30
num_episode_steps = 25



for delta in deltas:
    environment = Environment(display=False, magnification=500)

    agent = Agent(environment, DQN(), target_net=DQN(), gamma=gamma, batch_size=batch_size)

    # Policy arameters
    starting_epsilon = 1.0
    epsilon = None

    episodes_count, steps_count = 0, 0

    reward = 0
    # Loop over episodes
    while episodes_count < num_episodes:
        # Reset the environment for the start of the episode.
        agent.reset()
        episodes_count += 1
        # Loop over steps within this episode. The episode length here is 20.
        for step_num in range(num_episode_steps):
            steps_count += 1
            # Step the agent once, and get the transition tuple for this step
            transition = agent.step(epsilon=epsilon)
            agent.replaybuffer.append(transition)
            # Do not use e-greedy policy before training starts
            if steps_count == agent.batch_size - 1:
                epsilon = starting_epsilon
            # Do not start training until the ReplayBuffer has stored at least batch_size samples
            if steps_count >= batch_size:
                # Update target network weights
                if steps_count % update_target_every_steps == 0:
                    agent.target_net.q_network.load_state_dict(agent.dqn.q_network.state_dict())


                if epsilon != None:
                    # Decay epsilon by delta
                    epsilon = max(0.05, epsilon * delta)

                    # Decrease epsilon by delta
                    # epsilon = max(0.0, abs(epsilon - delta))

                # Sample a batch from the Replay Buffer
                batch = agent.replaybuffer.sample(size=agent.batch_size)
                agent.dqn.train_q_network(batch, agent.gamma)

        # Update total rewards
        reward += agent.total_reward / num_episodes

    rewards.append(reward)

# Plot loss for every step
plt.figure(figsize=(10,8))
plt.semilogy(deltas, rewards)
plt.xlabel(r'Decay Factor $\delta$'); plt.ylabel('Average Episode Total Sum of Rewards');
plt.title(r'Decay Factor $\delta$ vs Total Sum of Rewards ')
plt.show()

opt_delta = deltas[np.argmax(rewards)]

print('Optimal Delta: ', opt_delta)
