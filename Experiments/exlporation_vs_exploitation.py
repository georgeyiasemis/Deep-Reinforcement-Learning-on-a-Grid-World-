import sys
sys.path.insert(1, '../')

import numpy as np
import torch
import random
import matplotlib.pyplot as plt

# Import the environment module1
from environment import Environment
# Import agent and DQN
from agent import Agent
from deep_Qlearning import DQN


# =============================================================================
# Exploration vs Exploitation
# =============================================================================

seed = 0
np.random.seed(seed); torch.manual_seed(seed); random.seed(seed)


# Decay factor range
deltas = np.linspace(0, 1, 30)

# Store average rewards for each decay factor
rewards = []


gamma = 0.9
batch_size = 100

update_target_every_steps = 20

# Number of episodes
num_episodes = 50
num_episode_steps = 20



for i, delta in enumerate(deltas):
    environment = Environment(display=False, magnification=500, id=i)

    agent = Agent(environment, DQN(), target_net=DQN(), gamma=gamma, batch_size=batch_size)
    
    # Policy arameters
    starting_epsilon = None
    epsilon = None

    episodes_count, steps_count = 0, 0

    reward = 0
    # Loop over episodes
    while episodes_count < num_episodes:
        # Reset the environment for the start of the episode.
        agent.reset()
        episodes_count += 1
        # Loop over steps within this episode. The episode length here is 20.
        for step_num in range(num_episode_steps):
            steps_count += 1
            # Step the agent once, and get the transition tuple for this step
            transition = agent.step(epsilon=epsilon)
            agent.replaybuffer.append(transition)
            # Do not use e-greedy policy before training starts
            if steps_count == agent.batch_size - 1:
                epsilon = starting_epsilon
            # Do not start training until the ReplayBuffer has stored at least batch_size samples
            if steps_count >= batch_size:
                # Update target network weights
                if steps_count % update_target_every_steps == 0:
                    agent.target_net.q_network.load_state_dict(agent.dqn.q_network.state_dict())


                if epsilon != None:
                    # Decay epsilon by delta
                    epsilon = max(0.1, epsilon * delta)
                    
                    # Decrease epsilon by delta
                    # epsilon = max(0.0, abs(epsilon - delta))
                    
                # Sample a batch from the Replay Buffer
                batch = agent.replaybuffer.sample(size=agent.batch_size)
        # Update total rewards
        reward += agent.total_reward / num_episodes
        
    rewards.append(reward)
    
# Plot loss for every step
plt.figure(figsize=(10,8))
plt.semilogy(deltas, rewards)
plt.xlabel(r'Decay Factor $\delta$'); plt.ylabel('Average Episode Total Sum of Rewards');
plt.title(r'Decay Factor $\delta$ vs Total Sum of Rewards ')
plt.show()

opt_delta = deltas[np.argmax(rewards)]

print('Optimal Delta: ', opt_delta)
